{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Data_Predictive_Models.xlsx\", sheet_name = \"Data_Predictive_Models\")\n",
    "                #    \"Compiled_Data (2)\")\n",
    "# df\n",
    "\n",
    "# df = df.drop([\"Dates\"], axis = 1)\n",
    "heads = df.columns.to_list()\n",
    "# heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(method=\"bfill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = df.iloc[:,[1]]\n",
    "X1 = np.array(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categories='auto', drop=None, dtype=<class 'numpy.float64'>,\n",
       "              handle_unknown='error', sparse=True)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(categories='auto')\n",
    "enc.fit(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_enc = enc.transform(X1).toarray()\n",
    "X_tr = X_enc.transpose()\n",
    "X_tr.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_dict = {}\n",
    "for i in range(12):\n",
    "    enc_dict[i] = X_tr[i]\n",
    "\n",
    "for i in range(12):\n",
    "    df = pd.concat([df, pd.DataFrame(enc_dict[i], columns=[i])], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 1: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 2: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 3: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 4: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 5: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " 6: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 7: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 8: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 9: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 10: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 11: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads_new = [0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "for i in heads:\n",
    "    heads_new.append(i)\n",
    "\n",
    "heads_new\n",
    "\n",
    "df = df[heads_new]\n",
    "df = df.drop([\"Month\"], axis = 1)\n",
    "df = df.drop([\"DATE\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>RCTS</th>\n",
       "      <th>RTTS</th>\n",
       "      <th>AV</th>\n",
       "      <th>OTH</th>\n",
       "      <th>NHR</th>\n",
       "      <th>MTV</th>\n",
       "      <th>ENG</th>\n",
       "      <th>STL</th>\n",
       "      <th>TNR</th>\n",
       "      <th>AR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>290</td>\n",
       "      <td>1370</td>\n",
       "      <td>5.359190</td>\n",
       "      <td>182.5100</td>\n",
       "      <td>4.587156</td>\n",
       "      <td>71.0</td>\n",
       "      <td>79</td>\n",
       "      <td>34.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>340</td>\n",
       "      <td>1550</td>\n",
       "      <td>3.457004</td>\n",
       "      <td>33.8700</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>73.0</td>\n",
       "      <td>82</td>\n",
       "      <td>28.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>600</td>\n",
       "      <td>2490</td>\n",
       "      <td>4.146117</td>\n",
       "      <td>985.0900</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>70.0</td>\n",
       "      <td>83</td>\n",
       "      <td>28.0</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>528</td>\n",
       "      <td>2030</td>\n",
       "      <td>4.307066</td>\n",
       "      <td>1297.2000</td>\n",
       "      <td>4.464286</td>\n",
       "      <td>55.0</td>\n",
       "      <td>76</td>\n",
       "      <td>49.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>454</td>\n",
       "      <td>2070</td>\n",
       "      <td>4.311487</td>\n",
       "      <td>1724.6500</td>\n",
       "      <td>10.317460</td>\n",
       "      <td>65.0</td>\n",
       "      <td>82</td>\n",
       "      <td>49.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>473</td>\n",
       "      <td>1640</td>\n",
       "      <td>4.237779</td>\n",
       "      <td>1424.3400</td>\n",
       "      <td>2.439024</td>\n",
       "      <td>67.0</td>\n",
       "      <td>73</td>\n",
       "      <td>49.0</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>372</td>\n",
       "      <td>1335</td>\n",
       "      <td>5.086280</td>\n",
       "      <td>488.5400</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>73.0</td>\n",
       "      <td>86</td>\n",
       "      <td>27.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>540</td>\n",
       "      <td>1675</td>\n",
       "      <td>3.150410</td>\n",
       "      <td>69.1300</td>\n",
       "      <td>8.064516</td>\n",
       "      <td>65.0</td>\n",
       "      <td>87</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>475</td>\n",
       "      <td>1265</td>\n",
       "      <td>5.683258</td>\n",
       "      <td>10.7500</td>\n",
       "      <td>4.201681</td>\n",
       "      <td>49.0</td>\n",
       "      <td>78</td>\n",
       "      <td>45.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>545</td>\n",
       "      <td>1680</td>\n",
       "      <td>4.843364</td>\n",
       "      <td>47.5000</td>\n",
       "      <td>7.258065</td>\n",
       "      <td>67.0</td>\n",
       "      <td>79</td>\n",
       "      <td>27.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>408</td>\n",
       "      <td>1260</td>\n",
       "      <td>4.226931</td>\n",
       "      <td>23.7500</td>\n",
       "      <td>2.419355</td>\n",
       "      <td>58.0</td>\n",
       "      <td>83</td>\n",
       "      <td>38.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>488</td>\n",
       "      <td>965</td>\n",
       "      <td>3.907273</td>\n",
       "      <td>224.7800</td>\n",
       "      <td>1.626016</td>\n",
       "      <td>62.0</td>\n",
       "      <td>74</td>\n",
       "      <td>43.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>474</td>\n",
       "      <td>1195</td>\n",
       "      <td>2.787638</td>\n",
       "      <td>56.2500</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>65.0</td>\n",
       "      <td>77</td>\n",
       "      <td>39.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>459</td>\n",
       "      <td>1095</td>\n",
       "      <td>6.308538</td>\n",
       "      <td>330.0500</td>\n",
       "      <td>3.361345</td>\n",
       "      <td>56.0</td>\n",
       "      <td>84</td>\n",
       "      <td>30.0</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>416</td>\n",
       "      <td>1200</td>\n",
       "      <td>3.063341</td>\n",
       "      <td>711.4300</td>\n",
       "      <td>3.478261</td>\n",
       "      <td>80.0</td>\n",
       "      <td>81</td>\n",
       "      <td>26.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>582</td>\n",
       "      <td>1105</td>\n",
       "      <td>5.123488</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>10.483871</td>\n",
       "      <td>65.0</td>\n",
       "      <td>82</td>\n",
       "      <td>44.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>358</td>\n",
       "      <td>695</td>\n",
       "      <td>4.418402</td>\n",
       "      <td>6.7500</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>81</td>\n",
       "      <td>44.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>291</td>\n",
       "      <td>1345</td>\n",
       "      <td>2.565626</td>\n",
       "      <td>2040.5800</td>\n",
       "      <td>2.459016</td>\n",
       "      <td>69.0</td>\n",
       "      <td>81</td>\n",
       "      <td>36.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>312</td>\n",
       "      <td>1880</td>\n",
       "      <td>6.531408</td>\n",
       "      <td>2317.5000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>78.0</td>\n",
       "      <td>84</td>\n",
       "      <td>26.0</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>357</td>\n",
       "      <td>1525</td>\n",
       "      <td>4.947380</td>\n",
       "      <td>2424.0300</td>\n",
       "      <td>4.958678</td>\n",
       "      <td>78.0</td>\n",
       "      <td>81</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>336</td>\n",
       "      <td>1500</td>\n",
       "      <td>4.147923</td>\n",
       "      <td>1492.1900</td>\n",
       "      <td>8.730159</td>\n",
       "      <td>56.0</td>\n",
       "      <td>80</td>\n",
       "      <td>30.0</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>343</td>\n",
       "      <td>1758</td>\n",
       "      <td>2.865333</td>\n",
       "      <td>1625.7300</td>\n",
       "      <td>4.032258</td>\n",
       "      <td>51.0</td>\n",
       "      <td>72</td>\n",
       "      <td>45.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>450</td>\n",
       "      <td>1928</td>\n",
       "      <td>3.741304</td>\n",
       "      <td>1334.8600</td>\n",
       "      <td>1.639344</td>\n",
       "      <td>68.0</td>\n",
       "      <td>73</td>\n",
       "      <td>45.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>291</td>\n",
       "      <td>1911</td>\n",
       "      <td>5.614598</td>\n",
       "      <td>935.3500</td>\n",
       "      <td>0.813008</td>\n",
       "      <td>56.0</td>\n",
       "      <td>78</td>\n",
       "      <td>33.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>482</td>\n",
       "      <td>1690</td>\n",
       "      <td>3.966634</td>\n",
       "      <td>1732.2600</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>56.0</td>\n",
       "      <td>88</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.752294</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>483</td>\n",
       "      <td>2112</td>\n",
       "      <td>4.136394</td>\n",
       "      <td>1804.7000</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>70.0</td>\n",
       "      <td>87</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.464286</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>430</td>\n",
       "      <td>1849</td>\n",
       "      <td>4.375505</td>\n",
       "      <td>2462.0000</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>75.0</td>\n",
       "      <td>78</td>\n",
       "      <td>35.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>342</td>\n",
       "      <td>1543</td>\n",
       "      <td>2.211747</td>\n",
       "      <td>2382.4300</td>\n",
       "      <td>4.464286</td>\n",
       "      <td>65.0</td>\n",
       "      <td>80</td>\n",
       "      <td>29.0</td>\n",
       "      <td>5.357143</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>549</td>\n",
       "      <td>1564</td>\n",
       "      <td>6.231802</td>\n",
       "      <td>2973.9000</td>\n",
       "      <td>10.317460</td>\n",
       "      <td>69.0</td>\n",
       "      <td>88</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.968254</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>513</td>\n",
       "      <td>2120</td>\n",
       "      <td>4.682342</td>\n",
       "      <td>813.2700</td>\n",
       "      <td>2.439024</td>\n",
       "      <td>72.0</td>\n",
       "      <td>88</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.813008</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>515</td>\n",
       "      <td>1995</td>\n",
       "      <td>5.762570</td>\n",
       "      <td>1351.8000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>61.0</td>\n",
       "      <td>76</td>\n",
       "      <td>44.0</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>7.548966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>549</td>\n",
       "      <td>2465</td>\n",
       "      <td>4.321255</td>\n",
       "      <td>1069.8400</td>\n",
       "      <td>8.064516</td>\n",
       "      <td>79.0</td>\n",
       "      <td>87</td>\n",
       "      <td>34.0</td>\n",
       "      <td>4.838710</td>\n",
       "      <td>6.895424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>504</td>\n",
       "      <td>2220</td>\n",
       "      <td>3.874876</td>\n",
       "      <td>774.0500</td>\n",
       "      <td>4.201681</td>\n",
       "      <td>80.0</td>\n",
       "      <td>80</td>\n",
       "      <td>27.0</td>\n",
       "      <td>9.243697</td>\n",
       "      <td>8.457889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>580</td>\n",
       "      <td>2860</td>\n",
       "      <td>5.127459</td>\n",
       "      <td>1903.5600</td>\n",
       "      <td>7.258065</td>\n",
       "      <td>49.0</td>\n",
       "      <td>72</td>\n",
       "      <td>48.0</td>\n",
       "      <td>3.225806</td>\n",
       "      <td>10.032570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>505</td>\n",
       "      <td>2390</td>\n",
       "      <td>4.686399</td>\n",
       "      <td>1102.5500</td>\n",
       "      <td>2.419355</td>\n",
       "      <td>67.0</td>\n",
       "      <td>74</td>\n",
       "      <td>46.0</td>\n",
       "      <td>4.032258</td>\n",
       "      <td>9.865855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>475</td>\n",
       "      <td>1675</td>\n",
       "      <td>3.750247</td>\n",
       "      <td>253.5000</td>\n",
       "      <td>1.626016</td>\n",
       "      <td>69.0</td>\n",
       "      <td>78</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2.439024</td>\n",
       "      <td>8.655593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>380</td>\n",
       "      <td>2180</td>\n",
       "      <td>5.410936</td>\n",
       "      <td>947.6800</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>49.0</td>\n",
       "      <td>82</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>12.797619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>370</td>\n",
       "      <td>2095</td>\n",
       "      <td>3.750488</td>\n",
       "      <td>536.8500</td>\n",
       "      <td>3.361345</td>\n",
       "      <td>61.0</td>\n",
       "      <td>86</td>\n",
       "      <td>31.0</td>\n",
       "      <td>4.201681</td>\n",
       "      <td>8.489583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>375</td>\n",
       "      <td>2290</td>\n",
       "      <td>5.799243</td>\n",
       "      <td>875.3500</td>\n",
       "      <td>3.478261</td>\n",
       "      <td>68.0</td>\n",
       "      <td>79</td>\n",
       "      <td>32.0</td>\n",
       "      <td>6.956522</td>\n",
       "      <td>10.597826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>550</td>\n",
       "      <td>2545</td>\n",
       "      <td>4.942208</td>\n",
       "      <td>2315.5900</td>\n",
       "      <td>10.483871</td>\n",
       "      <td>69.0</td>\n",
       "      <td>79</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2.419355</td>\n",
       "      <td>9.325397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>520</td>\n",
       "      <td>2825</td>\n",
       "      <td>3.950403</td>\n",
       "      <td>1394.0100</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>53.0</td>\n",
       "      <td>74</td>\n",
       "      <td>39.0</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>7.291667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>500</td>\n",
       "      <td>2425</td>\n",
       "      <td>3.721212</td>\n",
       "      <td>1151.0500</td>\n",
       "      <td>2.459016</td>\n",
       "      <td>67.0</td>\n",
       "      <td>80</td>\n",
       "      <td>54.0</td>\n",
       "      <td>5.737705</td>\n",
       "      <td>6.912879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>465</td>\n",
       "      <td>2085</td>\n",
       "      <td>3.667945</td>\n",
       "      <td>610.9438</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>74.0</td>\n",
       "      <td>86</td>\n",
       "      <td>29.0</td>\n",
       "      <td>9.166667</td>\n",
       "      <td>7.604167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>380</td>\n",
       "      <td>2060</td>\n",
       "      <td>6.308538</td>\n",
       "      <td>722.7999</td>\n",
       "      <td>4.958678</td>\n",
       "      <td>60.0</td>\n",
       "      <td>83</td>\n",
       "      <td>38.0</td>\n",
       "      <td>4.132231</td>\n",
       "      <td>12.047101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>365</td>\n",
       "      <td>2160</td>\n",
       "      <td>3.927361</td>\n",
       "      <td>351.0000</td>\n",
       "      <td>8.730159</td>\n",
       "      <td>57.0</td>\n",
       "      <td>80</td>\n",
       "      <td>31.0</td>\n",
       "      <td>4.761905</td>\n",
       "      <td>7.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>445</td>\n",
       "      <td>2735</td>\n",
       "      <td>4.700447</td>\n",
       "      <td>581.8000</td>\n",
       "      <td>4.032258</td>\n",
       "      <td>57.0</td>\n",
       "      <td>79</td>\n",
       "      <td>33.0</td>\n",
       "      <td>8.064516</td>\n",
       "      <td>9.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>425</td>\n",
       "      <td>2555</td>\n",
       "      <td>4.289711</td>\n",
       "      <td>442.5000</td>\n",
       "      <td>1.639344</td>\n",
       "      <td>72.0</td>\n",
       "      <td>86</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3.278689</td>\n",
       "      <td>8.806818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>285</td>\n",
       "      <td>995</td>\n",
       "      <td>2.948996</td>\n",
       "      <td>55.7500</td>\n",
       "      <td>0.813008</td>\n",
       "      <td>77.0</td>\n",
       "      <td>82</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2.439024</td>\n",
       "      <td>5.539773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>490</td>\n",
       "      <td>2165</td>\n",
       "      <td>5.992117</td>\n",
       "      <td>206.6500</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>52.0</td>\n",
       "      <td>74</td>\n",
       "      <td>36.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>13.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>225</td>\n",
       "      <td>970</td>\n",
       "      <td>4.417304</td>\n",
       "      <td>162.5000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>64.0</td>\n",
       "      <td>75</td>\n",
       "      <td>33.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>9.611742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1    2    3    4    5    6    7    8    9  ...  RCTS  RTTS  \\\n",
       "0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   290  1370   \n",
       "1   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   340  1550   \n",
       "2   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   600  2490   \n",
       "3   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   528  2030   \n",
       "4   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   454  2070   \n",
       "5   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   473  1640   \n",
       "6   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   372  1335   \n",
       "7   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   540  1675   \n",
       "8   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   475  1265   \n",
       "9   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   545  1680   \n",
       "10  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   408  1260   \n",
       "11  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   488   965   \n",
       "12  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   474  1195   \n",
       "13  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  ...   459  1095   \n",
       "14  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   416  1200   \n",
       "15  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   582  1105   \n",
       "16  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   358   695   \n",
       "17  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...   291  1345   \n",
       "18  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   312  1880   \n",
       "19  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  ...   357  1525   \n",
       "20  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...   336  1500   \n",
       "21  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   343  1758   \n",
       "22  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   450  1928   \n",
       "23  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   291  1911   \n",
       "24  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   482  1690   \n",
       "25  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   483  2112   \n",
       "26  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   430  1849   \n",
       "27  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   342  1543   \n",
       "28  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   549  1564   \n",
       "29  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   513  2120   \n",
       "30  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   515  1995   \n",
       "31  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   549  2465   \n",
       "32  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   504  2220   \n",
       "33  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   580  2860   \n",
       "34  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   505  2390   \n",
       "35  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   475  1675   \n",
       "36  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...   380  2180   \n",
       "37  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   370  2095   \n",
       "38  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   375  2290   \n",
       "39  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   550  2545   \n",
       "40  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   520  2825   \n",
       "41  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   500  2425   \n",
       "42  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   465  2085   \n",
       "43  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   380  2060   \n",
       "44  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   365  2160   \n",
       "45  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   445  2735   \n",
       "46  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   425  2555   \n",
       "47  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   285   995   \n",
       "48  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  ...   490  2165   \n",
       "49  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   225   970   \n",
       "\n",
       "          AV        OTH        NHR   MTV  ENG   STL        TNR         AR  \n",
       "0   5.359190   182.5100   4.587156  71.0   79  34.0   4.000000  10.000000  \n",
       "1   3.457004    33.8700   6.250000  73.0   82  28.0   4.000000   8.000000  \n",
       "2   4.146117   985.0900   9.090909  70.0   83  28.0   9.000000   5.000000  \n",
       "3   4.307066  1297.2000   4.464286  55.0   76  49.0   4.000000   6.000000  \n",
       "4   4.311487  1724.6500  10.317460  65.0   82  49.0  10.000000   5.000000  \n",
       "5   4.237779  1424.3400   2.439024  67.0   73  49.0  11.000000   7.000000  \n",
       "6   5.086280   488.5400   3.333333  73.0   86  27.0   3.000000  11.000000  \n",
       "7   3.150410    69.1300   8.064516  65.0   87  33.0   1.000000  10.000000  \n",
       "8   5.683258    10.7500   4.201681  49.0   78  45.0   3.000000  11.000000  \n",
       "9   4.843364    47.5000   7.258065  67.0   79  27.0   8.000000   6.000000  \n",
       "10  4.226931    23.7500   2.419355  58.0   83  38.0   5.000000   8.000000  \n",
       "11  3.907273   224.7800   1.626016  62.0   74  43.0   5.000000  11.000000  \n",
       "12  2.787638    56.2500   1.666667  65.0   77  39.0   3.000000  12.000000  \n",
       "13  6.308538   330.0500   3.361345  56.0   84  30.0  11.000000   7.000000  \n",
       "14  3.063341   711.4300   3.478261  80.0   81  26.0  10.000000   5.000000  \n",
       "15  5.123488     0.0000  10.483871  65.0   82  44.0   5.000000   7.000000  \n",
       "16  4.418402     6.7500   4.800000  50.0   81  44.0   3.000000   7.000000  \n",
       "17  2.565626  2040.5800   2.459016  69.0   81  36.0   3.000000   9.000000  \n",
       "18  6.531408  2317.5000   5.000000  78.0   84  26.0  12.000000   6.000000  \n",
       "19  4.947380  2424.0300   4.958678  78.0   81  30.0   2.000000  10.000000  \n",
       "20  4.147923  1492.1900   8.730159  56.0   80  30.0   9.000000  11.000000  \n",
       "21  2.865333  1625.7300   4.032258  51.0   72  45.0   5.000000   6.000000  \n",
       "22  3.741304  1334.8600   1.639344  68.0   73  45.0   5.000000   5.000000  \n",
       "23  5.614598   935.3500   0.813008  56.0   78  33.0   5.000000  11.000000  \n",
       "24  3.966634  1732.2600   2.000000  56.0   88  30.0   2.752294   9.000000  \n",
       "25  4.136394  1804.7000   6.250000  70.0   87  30.0   4.464286  10.000000  \n",
       "26  4.375505  2462.0000   9.090909  75.0   78  35.0  10.000000   8.000000  \n",
       "27  2.211747  2382.4300   4.464286  65.0   80  29.0   5.357143   8.000000  \n",
       "28  6.231802  2973.9000  10.317460  69.0   88  32.0   3.968254   6.000000  \n",
       "29  4.682342   813.2700   2.439024  72.0   88  32.0   0.813008   7.000000  \n",
       "30  5.762570  1351.8000   3.333333  61.0   76  44.0   4.166667   7.548966  \n",
       "31  4.321255  1069.8400   8.064516  79.0   87  34.0   4.838710   6.895424  \n",
       "32  3.874876   774.0500   4.201681  80.0   80  27.0   9.243697   8.457889  \n",
       "33  5.127459  1903.5600   7.258065  49.0   72  48.0   3.225806  10.032570  \n",
       "34  4.686399  1102.5500   2.419355  67.0   74  46.0   4.032258   9.865855  \n",
       "35  3.750247   253.5000   1.626016  69.0   78  45.0   2.439024   8.655593  \n",
       "36  5.410936   947.6800   1.666667  49.0   82  52.0   4.166667  12.797619  \n",
       "37  3.750488   536.8500   3.361345  61.0   86  31.0   4.201681   8.489583  \n",
       "38  5.799243   875.3500   3.478261  68.0   79  32.0   6.956522  10.597826  \n",
       "39  4.942208  2315.5900  10.483871  69.0   79  34.0   2.419355   9.325397  \n",
       "40  3.950403  1394.0100   4.800000  53.0   74  39.0   4.800000   7.291667  \n",
       "41  3.721212  1151.0500   2.459016  67.0   80  54.0   5.737705   6.912879  \n",
       "42  3.667945   610.9438   5.000000  74.0   86  29.0   9.166667   7.604167  \n",
       "43  6.308538   722.7999   4.958678  60.0   83  38.0   4.132231  12.047101  \n",
       "44  3.927361   351.0000   8.730159  57.0   80  31.0   4.761905   7.812500  \n",
       "45  4.700447   581.8000   4.032258  57.0   79  33.0   8.064516   9.375000  \n",
       "46  4.289711   442.5000   1.639344  72.0   86  35.0   3.278689   8.806818  \n",
       "47  2.948996    55.7500   0.813008  77.0   82  34.0   2.439024   5.539773  \n",
       "48  5.992117   206.6500   2.000000  52.0   74  36.0   2.000000  13.020833  \n",
       "49  4.417304   162.5000   2.000000  64.0   75  33.0   3.000000   9.611742  \n",
       "\n",
       "[50 rows x 49 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"encoded.xlsx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process indicators (separate responses):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction for \"TR\":\n",
    "\n",
    "Y = df.iloc[:,[36]]\n",
    "X = df.iloc[:,0:35]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array(Y)\n",
    "X = np.array(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=47) # 80% training and 20% test\n",
    "\n",
    "print(y_train.shape)\n",
    "y_train = y_train.reshape((len(y_train),))\n",
    "print(y_train.shape)\n",
    "y_test = y_test.reshape((len(y_test),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, InputLayer, Conv1D, Dropout , Flatten, MaxPool1D, LeakyReLU\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.metrics import RootMeanSquaredError\n",
    "import warnings\n",
    "from keras.initializers import RandomUniform\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Neural Network Model\n",
    "class Neural_Network:\n",
    "    ### Argument of the class\n",
    "    # x_train : training input data\n",
    "    # y_train: training output data\n",
    "    # num_epochs: Number of epochs for learning NN\n",
    "\n",
    "\n",
    "    def __init__(self, x_train, y_train, num_epochs):\n",
    "        self.x_train_data = x_train\n",
    "        self.y_train = y_train\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "    ### Plotting the progress of NN learning with various errors\n",
    "    def plot_progress(self, model):\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1)  # MSE #MAE #MAPE\n",
    "        ax1.plot(model.epoch, model.history['root_mean_squared_error'], color='b')\n",
    "        ax1.set_xlabel(\"epoch\")\n",
    "        ax1.set_ylabel(\"Root Mean Squared Error\")\n",
    "\n",
    "        ax2.plot(model.epoch, model.history['mae'], color='r')\n",
    "        ax2.set_xlabel(\"epoch\")\n",
    "        ax2.set_ylabel(\"Mean Absolute Error\")\n",
    "\n",
    "        plt.rcParams['figure.figsize'] = [10, 10]\n",
    "        plt.subplots_adjust(left=0.1, bottom=0.1, right=0.95, top=0.95, wspace=0.55, hspace=0.1)\n",
    "\n",
    "        ## Saving the figure\n",
    "        plt.savefig(\"./fig_ANN.png\", dpi=300)\n",
    "\n",
    "    # Return the model\n",
    "    def model(self):\n",
    "        # Seed : to generate same random numbers each time\n",
    "        Seed_value = 73\n",
    "        keras.backend.clear_session()\n",
    "\n",
    "        ############### Define Our Neural Network Model ###############\n",
    "        model = Sequential()\n",
    "\n",
    "        ## First Hidden layer with relu as activation function and 5 as number of neurons\n",
    "        model.add(Dense(35, input_dim=self.x_train_data.shape[1], activation='relu',\n",
    "                        kernel_initializer=RandomUniform(seed=Seed_value)))\n",
    "\n",
    "        ## Second Hidden layer with relu as activation function and 15 as number of neurons\n",
    "        model.add(Dense(50, activation='softplus', kernel_initializer=RandomUniform(seed=Seed_value)))\n",
    "\n",
    "        ## Third Hidden layer with relu as activation function and 25 as number of neurons\n",
    "        model.add(Dense(100, activation='relu', kernel_initializer=RandomUniform(seed=Seed_value)))\n",
    "        \n",
    "        ## 4th Hidden layer with relu as activation function and 25 as number of neurons\n",
    "        model.add(Dense(200, activation='relu', kernel_initializer=RandomUniform(seed=Seed_value)))\n",
    "\n",
    "        ## Output Layer\n",
    "        model.add(Dense(1))\n",
    "        model.summary()\n",
    "\n",
    "\n",
    "        # Model Optimizer is ADAM with MSE, RMSE, and MAE errors.\n",
    "        model.compile('adam', loss='mse', metrics=[RootMeanSquaredError(), 'mae'])\n",
    "\n",
    "        # Learn the model using input/output training data\n",
    "        model_history = model.fit(self.x_train_data, self.y_train, epochs=self.num_epochs,\n",
    "                                  batch_size=32, verbose=0)\n",
    "\n",
    "        self.plot_progress(model_history)\n",
    "\n",
    "        # Return the model for further prediction on testing data\n",
    "        return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import preprocessing\n",
    "# import Stock_ANN\n",
    "\n",
    "if __name__ ==\"__main__\":\n",
    "\n",
    "#     # Get Training/Testing Data for all Indices\n",
    "\n",
    "\n",
    "#     selected_model = input(\"Please enter corresponding value to run Neural Network (1) or SVM (2):\\n\")\n",
    "    selected_model = 1\n",
    "    \n",
    "    ### Neural Network Model\n",
    "    if int(selected_model) == 1 :\n",
    "        ANN_instance = Neural_Network(X_train, y_train, num_epochs=1000)\n",
    "        ANN_prediction = ANN_instance.model().predict(X_test)\n",
    "        print(ANN_prediction.shape)\n",
    "        print(ANN_prediction[1:10])\n",
    "        y_pred = ANN_prediction\n",
    "\n",
    "    ### SVM Model\n",
    "    elif int(selected_model) == 2:\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        print(\"You Entered a Wrong Number\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## People Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction for \"TR\":\n",
    "# df = df.drop([\"\",\"\"], axis = 1)\n",
    "\n",
    "\n",
    "Y = df.iloc[:,[46]]\n",
    "X = df.iloc[:,0:46]\n",
    "Y = np.array(Y).astype(\"float16\")\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[34.],\n",
       "       [28.],\n",
       "       [28.],\n",
       "       [49.],\n",
       "       [49.],\n",
       "       [49.],\n",
       "       [27.],\n",
       "       [33.],\n",
       "       [45.],\n",
       "       [27.],\n",
       "       [38.],\n",
       "       [43.],\n",
       "       [39.],\n",
       "       [30.],\n",
       "       [26.],\n",
       "       [44.],\n",
       "       [44.],\n",
       "       [36.],\n",
       "       [26.],\n",
       "       [30.],\n",
       "       [30.],\n",
       "       [45.],\n",
       "       [45.],\n",
       "       [33.],\n",
       "       [30.],\n",
       "       [30.],\n",
       "       [35.],\n",
       "       [29.],\n",
       "       [32.],\n",
       "       [32.],\n",
       "       [44.],\n",
       "       [34.],\n",
       "       [27.],\n",
       "       [48.],\n",
       "       [46.],\n",
       "       [45.],\n",
       "       [52.],\n",
       "       [31.],\n",
       "       [32.],\n",
       "       [34.],\n",
       "       [39.],\n",
       "       [54.],\n",
       "       [29.],\n",
       "       [38.],\n",
       "       [31.],\n",
       "       [33.],\n",
       "       [35.],\n",
       "       [34.],\n",
       "       [36.],\n",
       "       [33.]], dtype=float16)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y= np.flip(Y, 0)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 1)\n",
      "(40,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=47) # 80% training and 20% test\n",
    "\n",
    "print(y_train.shape)\n",
    "y_train = y_train.reshape((len(y_train),))\n",
    "print(y_train.shape)\n",
    "y_test = y_test.reshape((len(y_test),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, InputLayer, Conv1D, Dropout , Flatten, MaxPool1D, LeakyReLU\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.metrics import RootMeanSquaredError\n",
    "import warnings\n",
    "from keras.initializers import RandomUniform\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Neural Network Model\n",
    "class Neural_Network:\n",
    "    ### Argument of the class\n",
    "    # x_train : training input data\n",
    "    # y_train: training output data\n",
    "    # num_epochs: Number of epochs for learning NN\n",
    "\n",
    "\n",
    "    def __init__(self, x_train, y_train, num_epochs):\n",
    "        self.x_train_data = x_train\n",
    "        self.y_train = y_train\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "    ### Plotting the progress of NN learning with various errors\n",
    "    def plot_progress(self, model):\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1)  # MSE #MAE #MAPE\n",
    "        ax1.plot(model.epoch, model.history['root_mean_squared_error'], color='b')\n",
    "        ax1.set_xlabel(\"epoch\")\n",
    "        ax1.set_ylabel(\"Root Mean Squared Error\")\n",
    "\n",
    "        ax2.plot(model.epoch, model.history['mae'], color='r')\n",
    "        ax2.set_xlabel(\"epoch\")\n",
    "        ax2.set_ylabel(\"Mean Absolute Error\")\n",
    "\n",
    "        plt.rcParams['figure.figsize'] = [10, 10]\n",
    "        plt.subplots_adjust(left=0.1, bottom=0.1, right=0.95, top=0.95, wspace=0.55, hspace=0.1)\n",
    "\n",
    "        ## Saving the figure\n",
    "        plt.savefig(\"./fig_ANN.png\", dpi=300)\n",
    "\n",
    "    # Return the model\n",
    "    def model(self):\n",
    "        # Seed : to generate same random numbers each time\n",
    "        Seed_value = 73\n",
    "        keras.backend.clear_session()\n",
    "\n",
    "        ############### Define Our Neural Network Model ###############\n",
    "        model = Sequential()\n",
    "\n",
    "        ## First Hidden layer with relu as activation function and 5 as number of neurons\n",
    "        model.add(Dense(46, input_dim=self.x_train_data.shape[1], activation='relu',\n",
    "                        kernel_initializer=RandomUniform(seed=Seed_value)))\n",
    "\n",
    "        ## Second Hidden layer with relu as activation function and 15 as number of neurons\n",
    "        model.add(Dense(100, activation='softplus', kernel_initializer=RandomUniform(seed=Seed_value)))\n",
    "\n",
    "        ## Third Hidden layer with relu as activation function and 25 as number of neurons\n",
    "        model.add(Dense(100, activation='relu', kernel_initializer=RandomUniform(seed=Seed_value)))\n",
    "        \n",
    "        ## 4th Hidden layer with relu as activation function and 25 as number of neurons\n",
    "        model.add(Dense(200, activation='relu', kernel_initializer=RandomUniform(seed=Seed_value)))\n",
    "\n",
    "        ## Output Layer\n",
    "        model.add(Dense(1))\n",
    "        model.summary()\n",
    "\n",
    "\n",
    "        # Model Optimizer is ADAM with MSE, RMSE, and MAE errors.\n",
    "        model.compile('adam', loss='mse', metrics=[RootMeanSquaredError(), 'mae'])\n",
    "\n",
    "        # Learn the model using input/output training data\n",
    "        model_history = model.fit(self.x_train_data, self.y_train, epochs=self.num_epochs,\n",
    "                                  batch_size=32, verbose=0)\n",
    "\n",
    "        self.plot_progress(model_history)\n",
    "\n",
    "        # Return the model for further prediction on testing data\n",
    "        return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 46)                2162      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               4700      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 37,363\n",
      "Trainable params: 37,363\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000218C61FE5E8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "(10, 1)\n",
      "[[40.166344]\n",
      " [33.274227]\n",
      " [25.774633]\n",
      " [63.53064 ]\n",
      " [45.43866 ]\n",
      " [30.585508]\n",
      " [30.066462]\n",
      " [26.95883 ]\n",
      " [43.406113]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import preprocessing\n",
    "# import Stock_ANN\n",
    "\n",
    "if __name__ ==\"__main__\":\n",
    "\n",
    "#     # Get Training/Testing Data for all Indices\n",
    "\n",
    "#     selected_model = input(\"Please enter corresponding value to run Neural Network (1) or SVM (2):\\n\")\n",
    "    selected_model = 1\n",
    "    \n",
    "    ### Neural Network Model\n",
    "    if int(selected_model) == 1 :\n",
    "        ANN_instance = Neural_Network(X_train, y_train, num_epochs=1000)\n",
    "        ANN_prediction = ANN_instance.model().predict(X_test)\n",
    "        print(ANN_prediction.shape)\n",
    "        print(ANN_prediction[1:10])\n",
    "        y_pred = ANN_prediction\n",
    "\n",
    "    ### SVM Model\n",
    "    elif int(selected_model) == 2:\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        print(\"You Entered a Wrong Number\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[44.67766 ],\n",
       "       [40.166344],\n",
       "       [33.274227],\n",
       "       [25.774633],\n",
       "       [63.53064 ],\n",
       "       [45.43866 ],\n",
       "       [30.585508],\n",
       "       [30.066462],\n",
       "       [26.95883 ],\n",
       "       [43.406113]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.560038624092617"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162.18417"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.110821"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ven610",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b71b3a05d14e972ac757b1afd52d6937e631e52b99a8b0bb96ec9afb5c1aecfa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
